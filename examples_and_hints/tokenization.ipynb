{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9826a2-4539-4922-a88d-3af38727c5fc",
   "metadata": {},
   "source": [
    "# Python Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fdfc84-70cb-46a9-aca1-7cd605f26550",
   "metadata": {},
   "source": [
    "## Tokenization using Python standard tokenizer + nltk\n",
    "You need to install nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f0c260-ae39-4fb5-ba08-33d5d0634962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_nlt(code):\n",
    "    try:\n",
    "        return word_tokenize(code.replace('`', '').replace(\"'\", ''))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "from tokenize import tokenize\n",
    "from io import BytesIO\n",
    "\n",
    "def tokenize_python(code):\n",
    "    \n",
    "    g = tokenize(BytesIO(code.encode('utf-8')).readline)\n",
    "    try:\n",
    "        tokens = [c[1] for c in g if c[1]!='' and c[1]!='\\n'][1:]\n",
    "    except:\n",
    "        tokens = tokenize_nlt(code)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        if ' ' in t:\n",
    "            clean_tokens += tokenize_nlt(t.replace('\"', '').replace(\"'\", ''))\n",
    "        else:\n",
    "            clean_tokens.append(t)\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e62d9b4f-60b3-4e1e-a3ed-10809ad601fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "x = 5.\n",
    "if x > 0:\n",
    "    print('x is positive')\n",
    "else:\n",
    "    print('x is negative')\n",
    "a.append(x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71cdb8e9-e000-4170-992d-f7ccb2869a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x',\n",
       " '=',\n",
       " '5.',\n",
       " 'if',\n",
       " 'x',\n",
       " '>',\n",
       " '0',\n",
       " ':',\n",
       " 'print',\n",
       " '(',\n",
       " 'x',\n",
       " 'is',\n",
       " 'positive',\n",
       " ')',\n",
       " 'else',\n",
       " ':',\n",
       " 'print',\n",
       " '(',\n",
       " 'x',\n",
       " 'is',\n",
       " 'negative',\n",
       " ')',\n",
       " 'a',\n",
       " '.',\n",
       " 'append',\n",
       " '(',\n",
       " 'x',\n",
       " ')']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_python(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7bc83f-8a05-4faf-98a6-2025a08f697b",
   "metadata": {},
   "source": [
    "## Tokenization using ByteLevelBPETokenizer\n",
    "\n",
    "Our pretrained model is available in the folder shared_resources/pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aafe708a-aa5d-483e-b738-9c80321c1fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'for', 'Ġx', 'Ġin', 'Ġrange', '(', '9', ')', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"../shared_resources/pretrained_tokenizer/py_tokenizer-vocab.json\",\n",
    "    \"../shared_resources/pretrained_tokenizer/py_tokenizer-merges.txt\",\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(\n",
    "    tokenizer.encode(\"for x in range(9)\").tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b0124-3e7c-4e18-b61e-c262da711004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
